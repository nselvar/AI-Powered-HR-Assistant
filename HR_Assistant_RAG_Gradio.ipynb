{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1e69d9",
   "metadata": {},
   "source": [
    "# AI-Powered HR Assistant (RAG + Gradio)\n",
    "\n",
    "This notebook builds an **HR Policy Q&A assistant** using:\n",
    "- PDF loading + chunking\n",
    "- Embeddings (SentenceTransformers)\n",
    "- FAISS vector search\n",
    "- Optional OpenAI chat model for final answers\n",
    "- Gradio UI\n",
    "\n",
    "**Input PDF:** `nestle_hr_policy.pdf` (upload to runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1) Install dependencies\n",
    "!pip -q install -U gradio pypdf sentence-transformers faiss-cpu openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d378c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2) Upload PDF (Colab only)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print('Uploaded:', list(uploaded.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158092ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 3) (Optional) Set OpenAI credentials\n",
    "import os\n",
    "\n",
    "# If using Vocareum-like OpenAI proxy:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = \"https://openai.vocareum.com/v1\"\n",
    "\n",
    "# If using standard OpenAI:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "\n",
    "# Optional model override:\n",
    "# os.environ[\"OPENAI_MODEL\"] = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d994750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 4) Load + chunk the PDF\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "\n",
    "PDF_PATH = \"nestle_hr_policy.pdf\"\n",
    "\n",
    "reader = PdfReader(PDF_PATH)\n",
    "\n",
    "raw_pages = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    txt = page.extract_text() or \"\"\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    raw_pages.append({\"page\": i + 1, \"text\": txt})\n",
    "\n",
    "print(\"Pages:\", len(raw_pages))\n",
    "print(\"Sample (page 1):\", raw_pages[0][\"text\"][:300], \"...\")\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "metas = []\n",
    "for p in raw_pages:\n",
    "    for ch in chunk_text(p[\"text\"]):\n",
    "        if ch.strip():\n",
    "            chunks.append(ch)\n",
    "            metas.append({\"page\": p[\"page\"]})\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"Sample chunk:\", chunks[0][:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 5) Build embeddings + FAISS (cosine similarity)\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# Normalize embeddings => inner product == cosine similarity\n",
    "emb = embed_model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dim = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(emb)\n",
    "\n",
    "print(\"Index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 6) Retrieval helper\n",
    "def retrieve(query: str, k: int = 4):\n",
    "    q = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, ids = index.search(q, k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], ids[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"text\": chunks[int(idx)],\n",
    "            \"page\": metas[int(idx)][\"page\"],\n",
    "            \"chunk_id\": int(idx),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "test_q = \"What does the policy say about harassment?\"\n",
    "res = retrieve(test_q, k=3)\n",
    "for r in res:\n",
    "    print(f\"Page {r['page']} | score={r['score']:.3f} | {r['text'][:120]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9af157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 7) (Optional) LLM answer with citations\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI() if os.getenv(\"OPENAI_API_KEY\") else None\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are an HR policy assistant. \"\n",
    "    \"Answer ONLY using the provided context. \"\n",
    "    \"If the answer is not in the context, say you don't know. \"\n",
    "    \"Keep answers clear and concise, and cite page numbers.\"\n",
    ")\n",
    "\n",
    "def answer_with_llm(question: str, k: int = 4):\n",
    "    retrieved = retrieve(question, k=k)\n",
    "    context = \"\\n\\n\".join([f\"(Page {r['page']}) {r['text']}\" for r in retrieved])\n",
    "\n",
    "    if client is None:\n",
    "        # Retrieval-only fallback\n",
    "        return (\n",
    "            \"LLM is disabled (no OPENAI_API_KEY set).\\n\\n\"\n",
    "            \"Top retrieved context:\\n\" + context[:2500],\n",
    "            retrieved,\n",
    "        )\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContext:\\n{context}\"},\n",
    "        ],\n",
    "    )\n",
    "    return resp.choices[0].message.content, retrieved\n",
    "\n",
    "ans, retrieved = answer_with_llm(\"What is the policy position on discrimination?\", k=4)\n",
    "print(ans)\n",
    "print(\"Sources:\", sorted({r[\"page\"] for r in retrieved}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 8) Gradio UI\n",
    "import gradio as gr\n",
    "\n",
    "def chat_fn(message, history, top_k):\n",
    "    answer, retrieved = answer_with_llm(message, k=int(top_k))\n",
    "    sources = \"\\n\".join([f\"- Page {r['page']} (score {r['score']:.3f})\" for r in retrieved])\n",
    "    return answer + \"\\n\\n---\\n**Sources (retrieved):**\\n\" + sources\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ“˜ HR Policy Assistant (RAG)\")\n",
    "    gr.Markdown(\"Ask questions about the uploaded HR policy PDF. The app retrieves relevant sections and (optionally) uses an LLM to answer.\")\n",
    "\n",
    "    top_k = gr.Slider(1, 8, value=4, step=1, label=\"Top-K chunks to retrieve\")\n",
    "    chat = gr.ChatInterface(fn=lambda msg, hist: chat_fn(msg, hist, top_k.value))\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
